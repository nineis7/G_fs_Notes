{
  /* layer index */
  "index":1,
  
  /* inter-layer dependency */
  // use layer index
  "input_layer":[0],
  "output_layer":[2],
  
  /* layer type */
  // 0 : fully-connection
  // 1 : normal conv2d
  // 2 : depthwise conv2d
  // 3 : single pooling/activation/residual
  // 4 : transposed conv2d
  // 5 : matrix-matrix multiplication (batch, nn.batch_matmul)
  // 6 : matrix-matrix multiplication (pack, nn.contrib_dense_pack)
  "type":6,
  // batch/pack size
  "group":8,
  
  /* shape */
  // output = operation(/*1st arg*/input, /*2nd arg*/ker)
  "input_size":[14, 3072],
  "output_size":[1, 14, 768],
  "weight_size":[96, 3072, 8],
  
  /* transpose */
  // whether and how tensors will be transposed before/after matrix-matrix multiplication (TBD)
  "input_transpose" : [],
  "weight_transpose" : [],
  "output_transpose" : [],

  /* reshape */
  // whether and how tensors will be reshaped before/after matrix-matrix multiplication (TBD)
  "input_reshape" : [],
  "output_reshape" : [],
  "after_transpose_reshape" : [],
  
  /* layout */
  // "data_layout":"BC",
  "weight_layout":"NC8n", // NC or NC8n 
  
  /* post processings -- element-wise operations */
  // pooling
  // "pool_padding_size":[0,0,0,0],  // padding ahead of pooling (not common)
  // "pooling_size":[1,2,2],
  // "pooling_stride":[1,2,2],
  // "pooling_type":1,  // 0 : no pooling, 1: max pooling, 2 : average pooling

  // layernorm
  // gamma and beta extract from constant?
  "epsilon":1e-12,

  // activation
  // 0 : no activation, 1 : ReLU, 2 : LeakyReLU, 3 : h-swish, 4 : h-sigmoid, 5 : GeLU (let's use 5 for now)
  "activation_type":0,  
  
  // residual add
  "residual":0,  // boolean
  "residual_source":[],  // layer index of residual source 
  
  // channel shuffle
  // "channel_shuffle":0,  // boolean
  
  // padding (conv only?)
  // "padding_size":[1,1,1,1],
  // "post_padding":1,
  // "post_padding_mode":1,
  // "post_padding_size":[1,1,1,1],
  // "pre_remove_padding_size":[0,0,0,0],
  // "extra_pre_padding":1,  // for 1st layer, extra: padding will be taken care offline rather than on-chip
  
  // ordering for post operations
  // 3 : after all
  // 2 : pool - res - relu(optional)
  // 1 : relu - res - pool(optional)
  // 0 : res - act - pool
  "res_position":3,
  // let's move this target-specific encoding to backend
  // use the new field below, a list of op name strings
  "post_ops" : ["nn.layer_norm", "nn.softmax"],
  
  // For certain layers as residual source layer, the output they provide to its next layer (which we name as main_output) is different from the residual output they provide;
  // For example, the main_output needs to go through the pooling process, therefore we have pooling 
  // parameters for current layer, but the residual output doesn't needs to go through the pooling process,
  // therefore, we need to separate this by the output choice:
  // -1, No output for this term. 
  // 0, no activation and pooling, direct output after the computation.
  // 1, go through pooling then output.
  // 2, go through activation then output.
  // 3. go through all the current layer setting.
  // For example, [3, -1] indicates a regular layer that doesn't act as a residual source, it will generate its
  // main_output based on current activation and pooling setting. 
  // [3, 2] indicates this layer act as a residual source, and its residual output only go through the activation 
  // process, then output. Even if there are pooling defined for the layer, the pooling will only be performed 
  // on the main_output.
  // Notice the there are also layer configured with [-1, 3], which means this layer doesn't connect to the 
  // main path, it only generate residual inputs for other layers.
  "output_choice":[3,-1],  

  /* quantization */
  // conv(ifm, weight/*static for inference*/) + bias
  // matrix-matrix multiplication(ifm, weight/*could be dynamic fm*/) + bias
  "input_fraclen":7,
  "weight_fraclen":2,
  "bias_fraclen":11,
  "output_fraclen":2,
  // let's add word length for each (was implicit)
  "input_word_length": 8,
  "weight_word_length": 8,
  "bias_word_length": 16,
  "output_word_length": 8
}
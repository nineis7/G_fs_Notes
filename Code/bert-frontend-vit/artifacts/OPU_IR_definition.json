{
  /* layer index */
  "index":1,
  
  /* inter-layer dependency */
  // use layer index
  "input_layer":[0],
  "output_layer":[2],
  
  /* layer type */
  // 0 : fully-connection
  // 1 : normal conv2d
  // 2 : depthwise conv2d
  // 3 : single pooling/activation/residual
  // 4 : transposed conv2d
  // 5 : matrix-matrix multiplication (let's use 5 for now)
  "type":1,
  // for group convolution
  "group":1,
  
  /* shape */
  // output = operation(/*1st arg*/input, /*2nd arg*/ker)
  "input_size":[1,418,418,3],
  "output_size":[1,210,210,16],
  "ker_size":[1,3,3],
  "ker_stride":[1,1,1],
  
  /* transpose */
  // whether and how tensors will be transposed before/after matrix-matrix multiplication (TBD)
  "input_transpose" : "",
  "ker_transpose" : "",
  "output_transpose" : "",
  
  /* layout */
  "data_layout":"NCHW",
  "kernel_layout":"OIHW",
  
  /* post processings -- element-wise operations */
  // pooling
  "pool_padding_size":[0,0,0,0],  // padding ahead of pooling (not common)
  "pooling_size":[1,2,2],
  "pooling_stride":[1,2,2],
  "pooling_type":1,  // 0 : no pooling, 1: max pooling, 2 : average pooling
  
  // activation
  // 0 : no activation, 1 : ReLU, 2 : LeakyReLU, 3 : h-swish, 4 : h-sigmoid, 5 : GeLU (let's use 5 for now)
  "activation_type":2,  
  
  // residual add
  "residual":0,  // boolean
  "residual_source":[],  // layer index of residual source 
  
  // upsampling
  "upsample":0,  // boolean
  "upsample_method":"",
  
  // channel shuffle
  "channel_shuffle":0,  // boolean
  
  // padding (conv only?)
  "padding_size":[1,1,1,1],
  "post_padding":1,
  "post_padding_mode":1,
  "post_padding_size":[1,1,1,1],
  "pre_remove_padding_size":[0,0,0,0],
  "extra_pre_padding":1,  // for 1st layer, extra: padding will be taken care offline rather than on-chip
  
  // ordering for post operations
  // 3 : after all
  // 2 : pool - res - relu(optional)
  // 1 : relu - res - pool(optional)
  // 0 : res - act - pool
  "res_position":3,
  // let's move this target-specific encoding to backend
  // use the new field below, a list of op name strings
  "post_ops" : ["relu", "pooling"],
  
  // For certain layers as residual source layer, the output they provide to its next layer (which we name as main_output) is different from the residual output they provide;
  // For example, the main_output needs to go through the pooling process, therefore we have pooling 
  // parameters for current layer, but the residual output doesn't needs to go through the pooling process,
  // therefore, we need to separate this by the output choice:
  // -1, No output for this term. 
  // 0, no activation and pooling, direct output after the computation.
  // 1, go through pooling then output.
  // 2, go through activation then output.
  // 3. go through all the current layer setting.
  // For example, [3, -1] indicates a regular layer that doesn't act as a residual source, it will generate its
  // main_output based on current activation and pooling setting. 
  // [3, 2] indicates this layer act as a residual source, and its residual output only go through the activation 
  // process, then output. Even if there are pooling defined for the layer, the pooling will only be performed 
  // on the main_output.
  // Notice the there are also layer configured with [-1, 3], which means this layer doesn't connect to the 
  // main path, it only generate residual inputs for other layers.
  "output_choice":[3,-1],  

  // not used
  "dilation":[1,1],
  
  /* quantization */
  // conv(ifm, weight/*static for inference*/) + bias
  // matrix-matrix multiplication(ifm, weight/*could be dynamic fm*/) + bias
  "input_fraclen":7,
  "weight_fraclen":2,
  "bias_fraclen":11,
  "output_fraclen":2
  // let's add word length for each (was implicit)
  "input_word_length": 8,
  "weight_word_length": 8,
  "bias_word_length": 16,
  "output_word_length": 8
}

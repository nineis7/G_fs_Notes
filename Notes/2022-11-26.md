# 动手学习深度学习

## numpy广播机制

1.  如果两个数组的维度相同（二维），对应位置上轴的长度全部相同，广播兼容，属于正常相加情况；
2. 如果两个数组的维度相同（二维），有一个轴长度相同且另一个的轴长度为1，广播兼容，可在轴长度为1的轴上进行广播机制处理；
3. 如果两个数组的维度相同（二维），对应位置轴长度均不相同，若两个数组交叉轴为1，则广播兼容，分别对两个长度为1的轴进行广播处理。

4. 如果两个数组的维度不同，那么给低维度的数组前扩展提升一维，扩展维的轴长度为1,然后在扩展出的维上进行(1)(2)(3)广播机制处理。
   -  (4)的情况也就是将高维数组的最高维当作是对剩下维的计数，例如reshape((2, 2, 3))，其中第一个2表示是有2个(2,3)shape的数组，所以低维数组的扩展其实就是把低维数组广播到这2个(2,3)数组上面，所以本质上仍然是低维广播规则的判断。

大佬发言：
> 抓住“右对齐”来理解广播机制是非常有好处的，判断任意tensor间是否可以广播，只需按照以下步骤就绝对不会出错了：
> 1. 将两操作对象的shape做右对齐
> 2. 空缺的位置假想为1
> 3. 比较同一位置处各操作对象的维数，若相同或有一个为1，则可以广播，否则无法广播
> 
> 例如两个tensor的shape分别为(8, 1, 6, 5)和 (7, 1, 5)，那么是否可以广播呢？
> 做右对齐, 空缺的位置假想为1:
> 8, 1, 6, 5
> 1, 7, 1, 5
> 按照以上规则得出是可以广播的，操作结果的shape应为(8, 7, 6, 5)

## 删除pandas缺失项最多的列

```python
dict_nan = {}
maxNum = -1
for col_name in ("Item", "Number", "Price"):
    dict_nan[col_name] = data[col_name].isnull().sum()
    if dict_nan[col_name] > maxNum:
        maxNum = dict_nan[col_name]
        maxCol = col_name

data_new = data.drop(maxCol, axis=1)
data_new
```

大佬写法
```python
x = data.isnull().sum()
'''
得到
Item      2
Number    1
Price     0
dtype: int64
'''
dfs = data.drop(x.index[x.argmax()],axis=1)
#dfs=df.drop(columns=x.index[x.argmax()])
'''
x.index 为Item，Number，Price（对应下标0，1，2）
x.argmax() 返回最大元素的index值，本例中为0
x.index[x.argmax()]即为'item'
'''
```

## 线性代数练习


1. 证明一个矩阵$\mathbf{A}$的转置的转置是$\mathbf{A}$，即$(\mathbf{A}^\top)^\top = \mathbf{A}$。
2. 给出两个矩阵$\mathbf{A}$和$\mathbf{B}$，证明“它们转置的和”等于“它们和的转置”，即$\mathbf{A}^\top + \mathbf{B}^\top = (\mathbf{A} + \mathbf{B})^\top$。
3. 给定任意方阵$\mathbf{A}$，$\mathbf{A} + \mathbf{A}^\top$总是对称的吗?为什么?
4. 我们在本节中定义了形状$(2,3,4)$的张量`X`。`len(X)`的输出结果是什么？
5. 对于任意形状的张量`X`,`len(X)`是否总是对应于`X`特定轴的长度?这个轴是什么?
6. 运行`A/A.sum(axis=1)`，看看会发生什么。你能分析原因吗？
7. 考虑一个具有形状$(2,3,4)$的张量，在轴0、1、2上的求和输出是什么形状?
8. 为`linalg.norm`函数提供3个或更多轴的张量，并观察其输出。对于任意形状的张量这个函数计算得到什么?


解答：
1. 转置是行列互换，互换再互换即不变。
2. 转置的和 和 和的转置，都是AB的行列的分别相加。
3. 记最终结果为B，B的ij为Aij+Aji，B的ji为Aji+Aij，对应位置的元素都相同，故对称。
4. len(X) 返回2，即第0轴的维数。
5. 看起来是第0轴；
6. A.sum(axis=1)降列而不降行,最终shape为5，行变成列，广播升维后5无法对应4; keep_dim后为(5,1)行对应行，可以广播；
7. (torch.Size([3, 4]), torch.Size([2, 4]), torch.Size([2, 3])) 即对哪个轴求和，该轴维数丢失；
8. 标量